# AI Voice Agent Overview

## Purpose and Scope
This document provides a comprehensive overview of the AI Voice Agent repository, a Streamlit-based application that transforms user voice input into AI-generated video responses. The system implements a sequential processing pipeline that combines speech recognition, knowledge retrieval, and video generation to create an interactive voice-to-video experience.

The AI Voice Agent serves as a demonstration of integrating multiple AI services into a cohesive user experience, showcasing real-time audio processing, vector database queries, and AI video synthesis. For detailed information about specific components, see Core Components. For external service integrations, see External Service Integrations.

## System Architecture and Core Functions
The AI Voice Agent follows a linear processing architecture implemented in the final.py module, where each user interaction triggers a sequence of transformations from audio input to video output.

## Main Processing Pipeline
![image](https://github.com/user-attachments/assets/a2027af8-2475-4faf-880d-5639221a1e6b)

## Core System Components

The system architecture centers around several key functional components that handle different aspects of the voice-to-video pipeline:

| Component          | Function Name            | Purpose                                   | Dependencies                     |
|--------------------|--------------------------|-------------------------------------------|----------------------------------|
| Audio Recording     | `audio_recorder()`       | Captures user voice input via Streamlit   | `audio_recorder_streamlit`       |
| Audio Storage       | `save_audio_file()`      | Persists audio data with timestamps       | File system                      |
| Speech Recognition  | `transcribe_to_text()`   | Converts audio to text transcript         | `speech_recognition`, Google API |
| Knowledge Retrieval | `query_answer()`         | Finds relevant responses from knowledge base | `chromadb`                    |
| Video Generation    | D-ID API calls           | Creates AI video from text responses      | D-ID external service            |
| Video Display       | `get_most_recent_video()`| Locates and displays generated videos     | File system, Streamlit           |

## Knowledge Base and Processing Flow
The application maintains a ChromaDB vector database that stores pre-defined knowledge about the user, enabling contextual responses to voice queries.

## ChromaDB Knowledge Architecture
![image](https://github.com/user-attachments/assets/45ff9c40-0f25-4143-9d4b-1237de718db1)

## Technology Stack and External Dependencies

The AI Voice Agent integrates multiple external services and libraries to deliver its functionality:

### Core Dependencies

| Technology               | Purpose              | Implementation Example                        |
|--------------------------|----------------------|------------------------------------------------|
| **Streamlit**            | Web UI framework     | `st.title()`, `st.button()`, `st.video()`      |
| **ChromaDB**             | Vector database      | `chromadb.Client()`, `collection.query()`      |
| **Speech Recognition**   | Audio transcription  | `sr.Recognizer()`, `r.recognize_google()`       |
| **D-ID API**             | AI video generation  | REST API calls with authentication             |
| **audio_recorder_streamlit** | Audio capture      | `audio_recorder()` component                  |

### External API Integration Flow

![image](https://github.com/user-attachments/assets/9784f57b-9ea6-49d9-9f90-b8b38924e8e3)

## File System Operations

The application manages several types of files during its operation, utilizing timestamp-based naming conventions for organization.

### File Management Functions

- **Audio Files**: Generated by `save_audio_file()` using the pattern `audio_YYYYMMDD_HHMMSS.mp3`
- **Transcripts**: Saved as `transcript.txt` for each processing session
- **Video Files**: Downloaded to the system **Downloads** folder and retrieved by `get_most_recent_video()`

### Processing Workflow

The complete user interaction workflow follows this sequence:

1. **Audio Capture**: User records voice via `audio_recorder()` component  
2. **File Storage**: Audio bytes saved via `save_audio_file()` with timestamp  
3. **Transcription**: Most recent audio file processed by `transcribe_to_text()`  
4. **Knowledge Query**: Transcript analyzed by `query_answer()` against ChromaDB  
5. **Video Generation**: Response sent to **D-ID API** for video creation  
6. **Result Display**: Generated video URL displayed and local video played  

### Sources

- `final.py` lines **7–12**
- `final.py` lines **14–21**
- `final.py` lines **59–136**

---

This architecture provides a complete end-to-end pipeline for transforming voice input into personalized AI video responses, demonstrating integration of multiple AI services within a web-based interface.
